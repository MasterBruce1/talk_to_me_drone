{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4edcd92a-6f29-4643-9c41-055e3841e2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Reshape, Conv1D, MaxPooling1D, Dropout, Flatten, Lambda\n",
    "from tensorflow.keras.optimizers import Adam  \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21a8c7e3-a281-4531-b57c-723340aeb843",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_from_folders(base_folder):\n",
    "    data = []\n",
    "    labels = []\n",
    "    ground_truths = [0, 30, 60, 90, 120, 150, 180, 210, 240, 270, 300, 330]\n",
    "    \n",
    "    for i, ground_truth in enumerate(ground_truths):\n",
    "        folder_path = os.path.join(base_folder, f'class_{i}')\n",
    "        for filename in os.listdir(folder_path):\n",
    "            if filename.endswith('.txt'):\n",
    "                file_path = os.path.join(folder_path, filename)\n",
    "                with open(file_path, 'r') as file:\n",
    "                    \n",
    "                    numbers = [float(num) for line in file for num in line.split(',')]\n",
    "                    if len(numbers) == 507:  \n",
    "                        data.append(numbers)\n",
    "                        labels.append(ground_truth)\n",
    "    return np.array(data), np.array(labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f7f961d-0196-45ea-8c1b-1ba9e2a40e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = 'B:\\jupyter_notebook\\localization'\n",
    "data, labels = load_data_from_folders(base_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c9f5e15-0ddd-44eb-95f1-1c472562a20d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30度的第一个样本数据: [ 1.3096e+02  2.0320e+01  1.9100e+00 -6.6300e+00 -2.3700e+00  4.0600e+00\n",
      "  2.8600e+00 -2.3800e+00 -3.0900e+00  1.4700e+00  2.8100e+00 -8.4000e-01\n",
      " -2.4900e+00  1.3085e+02  1.9610e+01  1.5100e+00 -7.7500e+00 -2.3500e+00\n",
      "  3.6400e+00  2.3200e+00 -1.8700e+00 -2.7800e+00  1.3900e+00  3.1600e+00\n",
      " -5.1000e-01 -2.0900e+00  1.2953e+02  1.9610e+01  2.1100e+00 -6.8600e+00\n",
      " -2.0500e+00  3.8800e+00  2.6200e+00 -2.6400e+00 -3.0800e+00  1.5400e+00\n",
      "  2.8200e+00 -9.3000e-01 -2.5000e+00  1.2978e+02  1.9630e+01  9.0000e-02\n",
      " -7.9600e+00 -3.1100e+00  3.5000e+00  2.6600e+00 -2.4300e+00 -2.4200e+00\n",
      "  1.6700e+00  3.3100e+00 -4.5000e-01 -2.7000e+00  1.2931e+02  1.9570e+01\n",
      "  8.4000e-01 -6.8100e+00 -2.3200e+00  3.0500e+00  1.7400e+00 -2.4500e+00\n",
      " -2.1900e+00  1.9800e+00  3.0100e+00 -8.8000e-01 -2.5400e+00  1.2875e+02\n",
      "  1.9900e+01  4.9000e-01 -7.3900e+00 -2.1700e+00  3.4700e+00  2.7100e+00\n",
      " -2.5400e+00 -2.2500e+00  1.1900e+00  2.8900e+00 -6.3000e-01 -2.4300e+00\n",
      "  1.2873e+02  1.9470e+01  1.0600e+00 -7.1200e+00 -2.5400e+00  4.8000e+00\n",
      "  2.8500e+00 -2.4700e+00 -2.5600e+00  1.3600e+00  2.8500e+00 -3.0000e-01\n",
      " -2.6100e+00  1.3025e+02  1.9620e+01  1.9700e+00 -7.2500e+00 -2.2700e+00\n",
      "  3.9000e+00  2.5100e+00 -2.7900e+00 -3.2900e+00  1.2700e+00  2.5700e+00\n",
      " -5.1000e-01 -2.4400e+00  1.2931e+02  2.0590e+01  9.0000e-01 -7.0600e+00\n",
      " -2.4000e+00  3.9500e+00  2.8100e+00 -3.3900e+00 -2.9800e+00  1.0100e+00\n",
      "  2.5800e+00 -3.2000e-01 -2.5800e+00  1.2988e+02  1.9760e+01  2.4800e+00\n",
      " -6.5500e+00 -2.0400e+00  4.2400e+00  2.7400e+00 -2.4100e+00 -2.9700e+00\n",
      "  1.6300e+00  2.7200e+00 -8.2000e-01 -2.7200e+00  1.2306e+02  1.8290e+01\n",
      "  1.2300e+00 -6.4800e+00 -2.5700e+00  4.0900e+00  2.4700e+00 -2.7000e+00\n",
      " -2.1300e+00  5.9000e-01  2.4000e+00 -1.0500e+00 -3.0000e+00  1.2513e+02\n",
      "  1.8750e+01  5.1000e-01 -7.2100e+00 -2.4500e+00  3.8400e+00  2.1500e+00\n",
      " -2.1400e+00 -2.8500e+00  9.5000e-01  2.4800e+00 -5.9000e-01 -2.5700e+00\n",
      "  1.2526e+02  1.9360e+01  1.2800e+00 -6.5700e+00 -1.8000e+00  3.3600e+00\n",
      "  3.0700e+00 -2.6800e+00 -2.7400e+00  1.4500e+00  2.7600e+00 -1.1000e+00\n",
      " -3.0100e+00  1.2544e+02  1.8960e+01  1.8600e+00 -7.0000e+00 -1.8200e+00\n",
      "  3.6300e+00  1.9300e+00 -2.2100e+00 -2.9300e+00  1.4100e+00  2.5700e+00\n",
      " -4.2000e-01 -2.8100e+00  1.2461e+02  1.8340e+01  1.5100e+00 -6.3700e+00\n",
      " -1.5600e+00  3.8000e+00  2.2700e+00 -2.9900e+00 -2.4300e+00  1.3100e+00\n",
      "  2.4800e+00 -5.3000e-01 -2.8400e+00  1.2534e+02  2.0060e+01  1.5100e+00\n",
      " -7.1400e+00 -1.7400e+00  4.4200e+00  2.6400e+00 -2.6800e+00 -1.8100e+00\n",
      "  1.0300e+00  2.2300e+00 -1.1300e+00 -3.2000e+00  1.2525e+02  1.8700e+01\n",
      "  1.0400e+00 -7.3300e+00 -2.7700e+00  3.6800e+00  2.1500e+00 -1.9100e+00\n",
      " -2.4900e+00  1.3400e+00  2.8100e+00 -5.3000e-01 -2.5400e+00  1.2671e+02\n",
      "  1.9350e+01  5.0000e-01 -6.5300e+00 -2.0900e+00  4.0800e+00  2.6000e+00\n",
      " -3.1300e+00 -2.3900e+00  1.6700e+00  2.6100e+00 -5.9000e-01 -2.9500e+00\n",
      "  1.2471e+02  1.8900e+01  1.2900e+00 -5.9600e+00 -2.3600e+00  4.8100e+00\n",
      "  1.6500e+00 -2.5500e+00 -2.5700e+00  1.6000e+00  2.6400e+00 -2.7000e-01\n",
      " -2.5600e+00  1.2377e+02  1.9210e+01  1.9200e+00 -7.8800e+00 -3.2200e+00\n",
      "  3.9800e+00  2.6300e+00 -2.6300e+00 -2.4100e+00  9.5000e-01  2.4300e+00\n",
      " -9.4000e-01 -2.3000e+00  1.2545e+02  1.9460e+01  1.9900e+00 -6.6300e+00\n",
      " -2.1700e+00  3.8200e+00  2.0400e+00 -2.9000e+00 -2.1700e+00  6.4000e-01\n",
      "  2.5900e+00 -9.4000e-01 -2.5800e+00  1.2560e+02  1.8430e+01  1.3000e+00\n",
      " -7.5000e+00 -2.3400e+00  3.7300e+00  3.1200e+00 -2.0900e+00 -2.4100e+00\n",
      "  1.1700e+00  2.7500e+00 -3.0000e-01 -2.4600e+00  1.2483e+02  1.9420e+01\n",
      "  1.4700e+00 -6.8800e+00 -1.8500e+00  4.3100e+00  2.3200e+00 -2.7600e+00\n",
      " -2.7300e+00  1.5800e+00  2.7500e+00 -7.3000e-01 -2.5600e+00  1.2615e+02\n",
      "  1.9260e+01  1.1300e+00 -6.7800e+00 -2.1200e+00  3.9000e+00  2.5600e+00\n",
      " -3.0300e+00 -3.2600e+00  9.1000e-01  2.8800e+00 -6.6000e-01 -2.5700e+00\n",
      "  1.2550e+02  1.9570e+01  3.9000e-01 -6.5000e+00 -2.0900e+00  4.1900e+00\n",
      "  2.8700e+00 -1.9900e+00 -2.1700e+00  1.6500e+00  2.9700e+00 -4.7000e-01\n",
      " -3.0400e+00  1.2459e+02  1.9680e+01  1.5400e+00 -6.1900e+00 -2.0700e+00\n",
      "  4.2800e+00  2.1600e+00 -3.2900e+00 -2.1100e+00  1.4000e+00  3.0300e+00\n",
      " -7.4000e-01 -2.9200e+00  1.2611e+02  1.9560e+01  1.9000e+00 -7.9800e+00\n",
      " -2.2700e+00  3.9700e+00  2.7500e+00 -2.4400e+00 -2.7500e+00  1.5700e+00\n",
      "  3.0600e+00 -2.0000e-01 -2.7900e+00  1.2426e+02  1.8750e+01  1.8000e+00\n",
      " -7.0300e+00 -2.3200e+00  4.2100e+00  3.2300e+00 -2.1400e+00 -2.1700e+00\n",
      "  1.2700e+00  2.7600e+00 -3.9000e-01 -2.7800e+00  1.2561e+02  1.9240e+01\n",
      "  2.2200e+00 -7.5400e+00 -1.9700e+00  4.4700e+00  2.9700e+00 -1.7700e+00\n",
      " -2.8300e+00  1.3500e+00  2.6400e+00 -8.8000e-01 -2.3300e+00  1.2598e+02\n",
      "  1.9090e+01  1.2500e+00 -7.3200e+00 -1.7000e+00  4.9900e+00  2.7700e+00\n",
      " -2.5100e+00 -2.3300e+00  1.5100e+00  2.5500e+00 -8.1000e-01 -2.7900e+00\n",
      "  1.2343e+02  1.9440e+01  1.0000e+00 -7.2000e+00 -1.7900e+00  3.8800e+00\n",
      "  3.2900e+00 -2.4600e+00 -2.2900e+00  1.4800e+00  2.4900e+00 -8.4000e-01\n",
      " -2.8600e+00  1.2487e+02  1.8440e+01  1.5900e+00 -6.9500e+00 -2.5700e+00\n",
      "  4.1400e+00  2.7500e+00 -3.0800e+00 -2.9400e+00  1.0500e+00  2.1700e+00\n",
      " -9.8000e-01 -2.8100e+00  1.2658e+02  1.9440e+01  1.3400e+00 -6.9600e+00\n",
      " -2.5700e+00  4.6100e+00  2.8300e+00 -2.1000e+00 -2.7300e+00  1.3100e+00\n",
      "  2.3400e+00 -8.6000e-01 -2.4800e+00  1.2587e+02  1.9280e+01  1.3200e+00\n",
      " -6.4300e+00 -2.7100e+00  3.7400e+00  2.9100e+00 -2.8700e+00 -2.0800e+00\n",
      "  1.5800e+00  3.0500e+00 -9.3000e-01 -2.5500e+00  1.2597e+02  1.9090e+01\n",
      "  1.2800e+00 -6.9600e+00 -3.4200e+00  3.6300e+00  2.5700e+00 -3.0200e+00\n",
      " -2.4000e+00  1.3800e+00  2.8500e+00 -9.9000e-01 -2.6000e+00  1.2662e+02\n",
      "  1.9490e+01  9.4000e-01 -7.0600e+00 -2.4200e+00  3.4100e+00  3.2100e+00\n",
      " -2.3500e+00 -2.6300e+00  1.0400e+00  2.2400e+00 -3.8000e-01 -2.4700e+00\n",
      "  1.2519e+02  1.9450e+01  1.0400e+00 -7.1100e+00 -1.9200e+00  3.4100e+00\n",
      "  2.6600e+00 -2.6400e+00 -2.8400e+00  1.4200e+00  2.2200e+00 -4.0000e-01\n",
      " -2.7200e+00  1.2609e+02  1.9020e+01  1.9200e+00 -7.4800e+00 -2.3400e+00\n",
      "  4.6300e+00  2.5100e+00 -3.3300e+00 -2.6700e+00  1.3900e+00  2.6600e+00\n",
      " -3.7000e-01 -2.6700e+00  1.2616e+02  2.0090e+01  1.7200e+00 -7.1600e+00\n",
      " -2.5800e+00  3.8500e+00  2.3600e+00 -2.8800e+00 -2.4900e+00  9.6000e-01\n",
      "  2.0700e+00 -2.2000e-01 -3.0200e+00]\n"
     ]
    }
   ],
   "source": [
    "index_of_30 = np.where(labels == 30)[0]  \n",
    "if len(index_of_30) > 0:\n",
    "    print(f\"30度的第一个样本数据: {data[index_of_30[0]]}\")\n",
    "else:\n",
    "    print(\"没有找到30度的样本\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c61b613b-33f9-4d69-8382-0cd3fcf9185c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_data, val_data, train_labels, val_labels = train_test_split(data, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eade3720-1418-41fa-81ce-98106c491806",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "BATCH_SIZE = 32\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_data, train_labels)).shuffle(buffer_size=len(train_data)).batch(BATCH_SIZE)\n",
    "validation_dataset = tf.data.Dataset.from_tensor_slices((val_data, val_labels)).batch(BATCH_SIZE)\n",
    "\n",
    "\n",
    "input_length = 507  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b2d9fbf-4bb9-440e-8d8e-457d370759a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cyclic_mse(y_true, y_pred):\n",
    "    pi = tf.constant(np.pi, dtype=tf.float32)\n",
    "    deg = tf.constant(180.0, dtype=tf.float32)  \n",
    "    y_true = tf.cast(y_true, dtype=tf.float32)  \n",
    "    y_pred = tf.cast(y_pred, dtype=tf.float32)  #y_pred to float32\n",
    "    return tf.reduce_mean(tf.square(\n",
    "        tf.atan2(\n",
    "            tf.sin(y_true * pi / deg - y_pred * pi / deg),\n",
    "            tf.cos(y_true * pi / deg - y_pred * pi / deg)\n",
    "        )\n",
    "    ) * deg / pi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "01f0cb4f-e77e-4cb1-9748-3a1fe417b068",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Reshape((int(input_length / 13), 13), input_shape=(input_length, )))\n",
    "model.add(Conv1D(8, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2, strides=2, padding='same'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv1D(16, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2, strides=2, padding='same'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1))  \n",
    "model.add(tf.keras.layers.Lambda(lambda x: x % 360))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a9bf5074-3459-4ed1-8913-10afdfa5e4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.005\n",
    "opt = Adam(learning_rate=LEARNING_RATE, beta_1=0.9, beta_2=0.999)\n",
    "model.compile(loss=cyclic_mse, optimizer=opt, metrics=[cyclic_mse])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c0278bf3-0d2c-4214-a11d-64f797a4ad4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "3/3 - 1s - 350ms/step - cyclic_mse: 187.9126 - loss: 190.6431 - val_cyclic_mse: 191.9204 - val_loss: 191.9204\n",
      "Epoch 2/300\n",
      "3/3 - 0s - 9ms/step - cyclic_mse: 186.8439 - loss: 200.3309 - val_cyclic_mse: 198.4471 - val_loss: 198.4471\n",
      "Epoch 3/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 186.0153 - loss: 191.9742 - val_cyclic_mse: 200.0546 - val_loss: 200.0546\n",
      "Epoch 4/300\n",
      "3/3 - 0s - 9ms/step - cyclic_mse: 184.6907 - loss: 179.9753 - val_cyclic_mse: 204.6755 - val_loss: 204.6755\n",
      "Epoch 5/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 183.0047 - loss: 190.2693 - val_cyclic_mse: 212.6091 - val_loss: 212.6091\n",
      "Epoch 6/300\n",
      "3/3 - 0s - 10ms/step - cyclic_mse: 182.7997 - loss: 184.1008 - val_cyclic_mse: 210.0609 - val_loss: 210.0609\n",
      "Epoch 7/300\n",
      "3/3 - 0s - 9ms/step - cyclic_mse: 185.8406 - loss: 190.4482 - val_cyclic_mse: 208.3343 - val_loss: 208.3343\n",
      "Epoch 8/300\n",
      "3/3 - 0s - 9ms/step - cyclic_mse: 183.3654 - loss: 174.2905 - val_cyclic_mse: 208.1041 - val_loss: 208.1041\n",
      "Epoch 9/300\n",
      "3/3 - 0s - 6ms/step - cyclic_mse: 184.5144 - loss: 182.3725 - val_cyclic_mse: 208.1174 - val_loss: 208.1174\n",
      "Epoch 10/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 182.6562 - loss: 189.3111 - val_cyclic_mse: 208.1127 - val_loss: 208.1127\n",
      "Epoch 11/300\n",
      "3/3 - 0s - 6ms/step - cyclic_mse: 183.0527 - loss: 180.9710 - val_cyclic_mse: 208.1257 - val_loss: 208.1257\n",
      "Epoch 12/300\n",
      "3/3 - 0s - 8ms/step - cyclic_mse: 184.8019 - loss: 187.5328 - val_cyclic_mse: 208.1664 - val_loss: 208.1664\n",
      "Epoch 13/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 183.6250 - loss: 178.2120 - val_cyclic_mse: 208.2513 - val_loss: 208.2513\n",
      "Epoch 14/300\n",
      "3/3 - 0s - 8ms/step - cyclic_mse: 183.0212 - loss: 183.2713 - val_cyclic_mse: 208.4543 - val_loss: 208.4543\n",
      "Epoch 15/300\n",
      "3/3 - 0s - 9ms/step - cyclic_mse: 184.0920 - loss: 179.7762 - val_cyclic_mse: 209.4867 - val_loss: 209.4867\n",
      "Epoch 16/300\n",
      "3/3 - 0s - 6ms/step - cyclic_mse: 184.3133 - loss: 181.5042 - val_cyclic_mse: 211.2962 - val_loss: 211.2963\n",
      "Epoch 17/300\n",
      "3/3 - 0s - 8ms/step - cyclic_mse: 185.3569 - loss: 190.2172 - val_cyclic_mse: 211.6898 - val_loss: 211.6898\n",
      "Epoch 18/300\n",
      "3/3 - 0s - 6ms/step - cyclic_mse: 184.3735 - loss: 184.6340 - val_cyclic_mse: 210.5727 - val_loss: 210.5727\n",
      "Epoch 19/300\n",
      "3/3 - 0s - 9ms/step - cyclic_mse: 184.1004 - loss: 183.7275 - val_cyclic_mse: 209.5128 - val_loss: 209.5129\n",
      "Epoch 20/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 183.8064 - loss: 186.4713 - val_cyclic_mse: 208.8933 - val_loss: 208.8933\n",
      "Epoch 21/300\n",
      "3/3 - 0s - 8ms/step - cyclic_mse: 182.8045 - loss: 183.1601 - val_cyclic_mse: 209.2247 - val_loss: 209.2248\n",
      "Epoch 22/300\n",
      "3/3 - 0s - 8ms/step - cyclic_mse: 183.1458 - loss: 178.9430 - val_cyclic_mse: 208.8931 - val_loss: 208.8931\n",
      "Epoch 23/300\n",
      "3/3 - 0s - 5ms/step - cyclic_mse: 183.2015 - loss: 194.0024 - val_cyclic_mse: 208.5728 - val_loss: 208.5728\n",
      "Epoch 24/300\n",
      "3/3 - 0s - 8ms/step - cyclic_mse: 183.5939 - loss: 177.3344 - val_cyclic_mse: 208.6354 - val_loss: 208.6354\n",
      "Epoch 25/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 182.7295 - loss: 170.5621 - val_cyclic_mse: 208.5901 - val_loss: 208.5901\n",
      "Epoch 26/300\n",
      "3/3 - 0s - 6ms/step - cyclic_mse: 183.6403 - loss: 180.4597 - val_cyclic_mse: 208.4030 - val_loss: 208.4030\n",
      "Epoch 27/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 183.3780 - loss: 174.2733 - val_cyclic_mse: 208.1099 - val_loss: 208.1099\n",
      "Epoch 28/300\n",
      "3/3 - 0s - 8ms/step - cyclic_mse: 180.9448 - loss: 181.0344 - val_cyclic_mse: 208.2183 - val_loss: 208.2183\n",
      "Epoch 29/300\n",
      "3/3 - 0s - 8ms/step - cyclic_mse: 182.7409 - loss: 168.1528 - val_cyclic_mse: 208.7220 - val_loss: 208.7220\n",
      "Epoch 30/300\n",
      "3/3 - 0s - 9ms/step - cyclic_mse: 182.9508 - loss: 195.5653 - val_cyclic_mse: 208.4754 - val_loss: 208.4754\n",
      "Epoch 31/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 182.2035 - loss: 178.2724 - val_cyclic_mse: 208.3297 - val_loss: 208.3297\n",
      "Epoch 32/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 183.1761 - loss: 199.2523 - val_cyclic_mse: 209.0942 - val_loss: 209.0942\n",
      "Epoch 33/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 183.4444 - loss: 186.6435 - val_cyclic_mse: 211.3802 - val_loss: 211.3802\n",
      "Epoch 34/300\n",
      "3/3 - 0s - 8ms/step - cyclic_mse: 182.6432 - loss: 168.4193 - val_cyclic_mse: 212.8004 - val_loss: 212.8004\n",
      "Epoch 35/300\n",
      "3/3 - 0s - 8ms/step - cyclic_mse: 181.7822 - loss: 182.4039 - val_cyclic_mse: 212.6191 - val_loss: 212.6191\n",
      "Epoch 36/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 182.3825 - loss: 183.0023 - val_cyclic_mse: 213.4728 - val_loss: 213.4728\n",
      "Epoch 37/300\n",
      "3/3 - 0s - 8ms/step - cyclic_mse: 182.6776 - loss: 176.9320 - val_cyclic_mse: 214.9224 - val_loss: 214.9224\n",
      "Epoch 38/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 183.6454 - loss: 167.3206 - val_cyclic_mse: 221.1264 - val_loss: 221.1264\n",
      "Epoch 39/300\n",
      "3/3 - 0s - 8ms/step - cyclic_mse: 186.6097 - loss: 179.1756 - val_cyclic_mse: 222.5957 - val_loss: 222.5957\n",
      "Epoch 40/300\n",
      "3/3 - 0s - 8ms/step - cyclic_mse: 182.7390 - loss: 176.0623 - val_cyclic_mse: 213.7978 - val_loss: 213.7978\n",
      "Epoch 41/300\n",
      "3/3 - 0s - 5ms/step - cyclic_mse: 182.4031 - loss: 183.5150 - val_cyclic_mse: 213.0928 - val_loss: 213.0928\n",
      "Epoch 42/300\n",
      "3/3 - 0s - 9ms/step - cyclic_mse: 182.5528 - loss: 186.5459 - val_cyclic_mse: 211.5364 - val_loss: 211.5364\n",
      "Epoch 43/300\n",
      "3/3 - 0s - 11ms/step - cyclic_mse: 181.7724 - loss: 194.2918 - val_cyclic_mse: 211.3794 - val_loss: 211.3794\n",
      "Epoch 44/300\n",
      "3/3 - 0s - 8ms/step - cyclic_mse: 185.3156 - loss: 192.5813 - val_cyclic_mse: 210.3676 - val_loss: 210.3676\n",
      "Epoch 45/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 182.7911 - loss: 180.1000 - val_cyclic_mse: 211.3836 - val_loss: 211.3836\n",
      "Epoch 46/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 183.9327 - loss: 178.1845 - val_cyclic_mse: 211.6663 - val_loss: 211.6663\n",
      "Epoch 47/300\n",
      "3/3 - 0s - 6ms/step - cyclic_mse: 182.7143 - loss: 172.5339 - val_cyclic_mse: 211.8820 - val_loss: 211.8819\n",
      "Epoch 48/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 181.2142 - loss: 195.3680 - val_cyclic_mse: 211.5535 - val_loss: 211.5535\n",
      "Epoch 49/300\n",
      "3/3 - 0s - 5ms/step - cyclic_mse: 182.3560 - loss: 181.3849 - val_cyclic_mse: 211.3953 - val_loss: 211.3953\n",
      "Epoch 50/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 182.3053 - loss: 194.1568 - val_cyclic_mse: 211.3788 - val_loss: 211.3788\n",
      "Epoch 51/300\n",
      "3/3 - 0s - 8ms/step - cyclic_mse: 185.5933 - loss: 192.6963 - val_cyclic_mse: 209.2927 - val_loss: 209.2927\n",
      "Epoch 52/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 182.8311 - loss: 181.2601 - val_cyclic_mse: 209.0509 - val_loss: 209.0509\n",
      "Epoch 53/300\n",
      "3/3 - 0s - 9ms/step - cyclic_mse: 182.5277 - loss: 182.7511 - val_cyclic_mse: 209.2081 - val_loss: 209.2081\n",
      "Epoch 54/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 183.4922 - loss: 192.4563 - val_cyclic_mse: 208.5795 - val_loss: 208.5795\n",
      "Epoch 55/300\n",
      "3/3 - 0s - 8ms/step - cyclic_mse: 183.7935 - loss: 182.6066 - val_cyclic_mse: 208.3719 - val_loss: 208.3719\n",
      "Epoch 56/300\n",
      "3/3 - 0s - 6ms/step - cyclic_mse: 182.5390 - loss: 180.6083 - val_cyclic_mse: 209.0329 - val_loss: 209.0329\n",
      "Epoch 57/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 183.0291 - loss: 185.0332 - val_cyclic_mse: 211.1120 - val_loss: 211.1121\n",
      "Epoch 58/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 189.0538 - loss: 190.2715 - val_cyclic_mse: 212.4776 - val_loss: 212.4776\n",
      "Epoch 59/300\n",
      "3/3 - 0s - 6ms/step - cyclic_mse: 183.0651 - loss: 193.3949 - val_cyclic_mse: 215.4776 - val_loss: 215.4776\n",
      "Epoch 60/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 181.3006 - loss: 171.0569 - val_cyclic_mse: 219.6300 - val_loss: 219.6300\n",
      "Epoch 61/300\n",
      "3/3 - 0s - 6ms/step - cyclic_mse: 180.5675 - loss: 191.6864 - val_cyclic_mse: 217.7488 - val_loss: 217.7488\n",
      "Epoch 62/300\n",
      "3/3 - 0s - 6ms/step - cyclic_mse: 183.2577 - loss: 180.5971 - val_cyclic_mse: 216.1456 - val_loss: 216.1456\n",
      "Epoch 63/300\n",
      "3/3 - 0s - 6ms/step - cyclic_mse: 179.0063 - loss: 175.0046 - val_cyclic_mse: 216.0589 - val_loss: 216.0589\n",
      "Epoch 64/300\n",
      "3/3 - 0s - 6ms/step - cyclic_mse: 180.5009 - loss: 177.1670 - val_cyclic_mse: 216.7886 - val_loss: 216.7887\n",
      "Epoch 65/300\n",
      "3/3 - 0s - 6ms/step - cyclic_mse: 180.8971 - loss: 181.3301 - val_cyclic_mse: 217.1468 - val_loss: 217.1468\n",
      "Epoch 66/300\n",
      "3/3 - 0s - 6ms/step - cyclic_mse: 182.1467 - loss: 190.9586 - val_cyclic_mse: 218.3465 - val_loss: 218.3465\n",
      "Epoch 67/300\n",
      "3/3 - 0s - 6ms/step - cyclic_mse: 182.8018 - loss: 172.6909 - val_cyclic_mse: 216.0987 - val_loss: 216.0986\n",
      "Epoch 68/300\n",
      "3/3 - 0s - 6ms/step - cyclic_mse: 181.0051 - loss: 179.7224 - val_cyclic_mse: 215.7513 - val_loss: 215.7513\n",
      "Epoch 69/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 182.7545 - loss: 180.8644 - val_cyclic_mse: 216.9723 - val_loss: 216.9723\n",
      "Epoch 70/300\n",
      "3/3 - 0s - 6ms/step - cyclic_mse: 182.2631 - loss: 186.4967 - val_cyclic_mse: 218.8260 - val_loss: 218.8260\n",
      "Epoch 71/300\n",
      "3/3 - 0s - 6ms/step - cyclic_mse: 182.4951 - loss: 180.1210 - val_cyclic_mse: 219.8478 - val_loss: 219.8479\n",
      "Epoch 72/300\n",
      "3/3 - 0s - 6ms/step - cyclic_mse: 183.0365 - loss: 194.3525 - val_cyclic_mse: 221.8595 - val_loss: 221.8595\n",
      "Epoch 73/300\n",
      "3/3 - 0s - 6ms/step - cyclic_mse: 184.5164 - loss: 181.1069 - val_cyclic_mse: 224.5668 - val_loss: 224.5668\n",
      "Epoch 74/300\n",
      "3/3 - 0s - 6ms/step - cyclic_mse: 182.1880 - loss: 193.5876 - val_cyclic_mse: 225.6932 - val_loss: 225.6932\n",
      "Epoch 75/300\n",
      "3/3 - 0s - 6ms/step - cyclic_mse: 180.9145 - loss: 174.9576 - val_cyclic_mse: 224.2863 - val_loss: 224.2863\n",
      "Epoch 76/300\n",
      "3/3 - 0s - 6ms/step - cyclic_mse: 182.8993 - loss: 171.1370 - val_cyclic_mse: 222.6765 - val_loss: 222.6765\n",
      "Epoch 77/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 183.9521 - loss: 198.8096 - val_cyclic_mse: 213.7785 - val_loss: 213.7785\n",
      "Epoch 78/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 181.0051 - loss: 179.3957 - val_cyclic_mse: 212.0270 - val_loss: 212.0270\n",
      "Epoch 79/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 182.6091 - loss: 183.8078 - val_cyclic_mse: 211.3775 - val_loss: 211.3775\n",
      "Epoch 80/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 182.4961 - loss: 175.9622 - val_cyclic_mse: 209.1155 - val_loss: 209.1155\n",
      "Epoch 81/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 182.1364 - loss: 185.4763 - val_cyclic_mse: 208.4096 - val_loss: 208.4096\n",
      "Epoch 82/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 184.7847 - loss: 188.5654 - val_cyclic_mse: 208.2193 - val_loss: 208.2194\n",
      "Epoch 83/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 183.4651 - loss: 181.5001 - val_cyclic_mse: 208.2380 - val_loss: 208.2380\n",
      "Epoch 84/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 182.7391 - loss: 181.8955 - val_cyclic_mse: 208.1926 - val_loss: 208.1926\n",
      "Epoch 85/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 182.9932 - loss: 184.1783 - val_cyclic_mse: 208.2176 - val_loss: 208.2177\n",
      "Epoch 86/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 182.9159 - loss: 176.8850 - val_cyclic_mse: 208.1519 - val_loss: 208.1519\n",
      "Epoch 87/300\n",
      "3/3 - 0s - 6ms/step - cyclic_mse: 183.6293 - loss: 186.6133 - val_cyclic_mse: 208.1720 - val_loss: 208.1720\n",
      "Epoch 88/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 182.8716 - loss: 183.1945 - val_cyclic_mse: 208.1699 - val_loss: 208.1699\n",
      "Epoch 89/300\n",
      "3/3 - 0s - 6ms/step - cyclic_mse: 183.0398 - loss: 188.3229 - val_cyclic_mse: 208.2448 - val_loss: 208.2448\n",
      "Epoch 90/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 182.2120 - loss: 180.3797 - val_cyclic_mse: 208.5044 - val_loss: 208.5044\n",
      "Epoch 91/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 183.3420 - loss: 178.3504 - val_cyclic_mse: 209.2542 - val_loss: 209.2542\n",
      "Epoch 92/300\n",
      "3/3 - 0s - 6ms/step - cyclic_mse: 182.8673 - loss: 175.9731 - val_cyclic_mse: 210.0124 - val_loss: 210.0124\n",
      "Epoch 93/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 181.4338 - loss: 181.2051 - val_cyclic_mse: 210.4552 - val_loss: 210.4552\n",
      "Epoch 94/300\n",
      "3/3 - 0s - 5ms/step - cyclic_mse: 180.8605 - loss: 194.4392 - val_cyclic_mse: 209.9702 - val_loss: 209.9701\n",
      "Epoch 95/300\n",
      "3/3 - 0s - 6ms/step - cyclic_mse: 183.2273 - loss: 183.4224 - val_cyclic_mse: 209.4783 - val_loss: 209.4783\n",
      "Epoch 96/300\n",
      "3/3 - 0s - 6ms/step - cyclic_mse: 179.8057 - loss: 174.7649 - val_cyclic_mse: 209.4783 - val_loss: 209.4783\n",
      "Epoch 97/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 183.8779 - loss: 176.9254 - val_cyclic_mse: 209.0968 - val_loss: 209.0968\n",
      "Epoch 98/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 182.3122 - loss: 184.0971 - val_cyclic_mse: 209.1321 - val_loss: 209.1321\n",
      "Epoch 99/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 182.0400 - loss: 188.1796 - val_cyclic_mse: 209.6576 - val_loss: 209.6576\n",
      "Epoch 100/300\n",
      "3/3 - 0s - 8ms/step - cyclic_mse: 180.6775 - loss: 185.3910 - val_cyclic_mse: 210.6041 - val_loss: 210.6041\n",
      "Epoch 101/300\n",
      "3/3 - 0s - 8ms/step - cyclic_mse: 181.7031 - loss: 190.4595 - val_cyclic_mse: 211.3973 - val_loss: 211.3973\n",
      "Epoch 102/300\n",
      "3/3 - 0s - 6ms/step - cyclic_mse: 183.1544 - loss: 179.0872 - val_cyclic_mse: 211.2249 - val_loss: 211.2249\n",
      "Epoch 103/300\n",
      "3/3 - 0s - 9ms/step - cyclic_mse: 181.6660 - loss: 177.0919 - val_cyclic_mse: 210.5554 - val_loss: 210.5554\n",
      "Epoch 104/300\n",
      "3/3 - 0s - 8ms/step - cyclic_mse: 181.5026 - loss: 183.6500 - val_cyclic_mse: 210.2336 - val_loss: 210.2336\n",
      "Epoch 105/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 185.0788 - loss: 176.7693 - val_cyclic_mse: 210.7808 - val_loss: 210.7808\n",
      "Epoch 106/300\n",
      "3/3 - 0s - 6ms/step - cyclic_mse: 182.5865 - loss: 180.9012 - val_cyclic_mse: 209.7383 - val_loss: 209.7382\n",
      "Epoch 107/300\n",
      "3/3 - 0s - 6ms/step - cyclic_mse: 181.2746 - loss: 178.8425 - val_cyclic_mse: 209.7241 - val_loss: 209.7240\n",
      "Epoch 108/300\n",
      "3/3 - 0s - 6ms/step - cyclic_mse: 183.8558 - loss: 184.1044 - val_cyclic_mse: 209.4755 - val_loss: 209.4755\n",
      "Epoch 109/300\n",
      "3/3 - 0s - 6ms/step - cyclic_mse: 182.4847 - loss: 187.2507 - val_cyclic_mse: 209.6469 - val_loss: 209.6469\n",
      "Epoch 110/300\n",
      "3/3 - 0s - 6ms/step - cyclic_mse: 181.7528 - loss: 184.9311 - val_cyclic_mse: 210.1850 - val_loss: 210.1850\n",
      "Epoch 111/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 180.6525 - loss: 183.2149 - val_cyclic_mse: 210.5409 - val_loss: 210.5409\n",
      "Epoch 112/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 183.3539 - loss: 191.2760 - val_cyclic_mse: 210.8362 - val_loss: 210.8362\n",
      "Epoch 113/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 182.2860 - loss: 193.7955 - val_cyclic_mse: 210.4971 - val_loss: 210.4971\n",
      "Epoch 114/300\n",
      "3/3 - 0s - 6ms/step - cyclic_mse: 182.8616 - loss: 184.8569 - val_cyclic_mse: 210.2536 - val_loss: 210.2535\n",
      "Epoch 115/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 182.4308 - loss: 182.3067 - val_cyclic_mse: 210.1615 - val_loss: 210.1615\n",
      "Epoch 116/300\n",
      "3/3 - 0s - 6ms/step - cyclic_mse: 182.0658 - loss: 185.5885 - val_cyclic_mse: 209.4033 - val_loss: 209.4033\n",
      "Epoch 117/300\n",
      "3/3 - 0s - 8ms/step - cyclic_mse: 181.6389 - loss: 178.9979 - val_cyclic_mse: 209.2549 - val_loss: 209.2548\n",
      "Epoch 118/300\n",
      "3/3 - 0s - 11ms/step - cyclic_mse: 182.0861 - loss: 183.8619 - val_cyclic_mse: 209.0746 - val_loss: 209.0746\n",
      "Epoch 119/300\n",
      "3/3 - 0s - 8ms/step - cyclic_mse: 184.3982 - loss: 180.4005 - val_cyclic_mse: 209.3304 - val_loss: 209.3304\n",
      "Epoch 120/300\n",
      "3/3 - 0s - 6ms/step - cyclic_mse: 182.3881 - loss: 186.1200 - val_cyclic_mse: 209.3672 - val_loss: 209.3672\n",
      "Epoch 121/300\n",
      "3/3 - 0s - 8ms/step - cyclic_mse: 183.8014 - loss: 182.9712 - val_cyclic_mse: 209.6350 - val_loss: 209.6351\n",
      "Epoch 122/300\n",
      "3/3 - 0s - 6ms/step - cyclic_mse: 182.0041 - loss: 189.5570 - val_cyclic_mse: 210.5210 - val_loss: 210.5210\n",
      "Epoch 123/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 181.7223 - loss: 179.4306 - val_cyclic_mse: 210.9173 - val_loss: 210.9173\n",
      "Epoch 124/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 182.4803 - loss: 185.7647 - val_cyclic_mse: 211.3927 - val_loss: 211.3927\n",
      "Epoch 125/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 181.9163 - loss: 189.9180 - val_cyclic_mse: 211.3877 - val_loss: 211.3877\n",
      "Epoch 126/300\n",
      "3/3 - 0s - 6ms/step - cyclic_mse: 180.7333 - loss: 179.4860 - val_cyclic_mse: 210.8636 - val_loss: 210.8636\n",
      "Epoch 127/300\n",
      "3/3 - 0s - 6ms/step - cyclic_mse: 181.4953 - loss: 178.2243 - val_cyclic_mse: 210.6026 - val_loss: 210.6026\n",
      "Epoch 128/300\n",
      "3/3 - 0s - 6ms/step - cyclic_mse: 181.6775 - loss: 184.8840 - val_cyclic_mse: 210.3389 - val_loss: 210.3389\n",
      "Epoch 129/300\n",
      "3/3 - 0s - 6ms/step - cyclic_mse: 180.8760 - loss: 177.2194 - val_cyclic_mse: 211.2066 - val_loss: 211.2066\n",
      "Epoch 130/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 181.5455 - loss: 179.5470 - val_cyclic_mse: 211.3758 - val_loss: 211.3757\n",
      "Epoch 131/300\n",
      "3/3 - 0s - 6ms/step - cyclic_mse: 180.6322 - loss: 185.8885 - val_cyclic_mse: 211.4126 - val_loss: 211.4126\n",
      "Epoch 132/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 181.5692 - loss: 175.8763 - val_cyclic_mse: 211.8050 - val_loss: 211.8050\n",
      "Epoch 133/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 181.7103 - loss: 182.7989 - val_cyclic_mse: 213.2860 - val_loss: 213.2860\n",
      "Epoch 134/300\n",
      "3/3 - 0s - 6ms/step - cyclic_mse: 181.5463 - loss: 181.9716 - val_cyclic_mse: 212.7165 - val_loss: 212.7165\n",
      "Epoch 135/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 181.8211 - loss: 178.5886 - val_cyclic_mse: 213.9816 - val_loss: 213.9816\n",
      "Epoch 136/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 181.6925 - loss: 186.7741 - val_cyclic_mse: 214.2316 - val_loss: 214.2316\n",
      "Epoch 137/300\n",
      "3/3 - 0s - 8ms/step - cyclic_mse: 181.6608 - loss: 177.6381 - val_cyclic_mse: 213.5964 - val_loss: 213.5964\n",
      "Epoch 138/300\n",
      "3/3 - 0s - 8ms/step - cyclic_mse: 181.7995 - loss: 177.1967 - val_cyclic_mse: 212.2332 - val_loss: 212.2332\n",
      "Epoch 139/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 181.4922 - loss: 183.3930 - val_cyclic_mse: 211.6779 - val_loss: 211.6779\n",
      "Epoch 140/300\n",
      "3/3 - 0s - 8ms/step - cyclic_mse: 181.6774 - loss: 178.0433 - val_cyclic_mse: 211.5805 - val_loss: 211.5805\n",
      "Epoch 141/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 181.1541 - loss: 187.4838 - val_cyclic_mse: 211.7784 - val_loss: 211.7784\n",
      "Epoch 142/300\n",
      "3/3 - 0s - 8ms/step - cyclic_mse: 182.6621 - loss: 190.6944 - val_cyclic_mse: 212.6758 - val_loss: 212.6758\n",
      "Epoch 143/300\n",
      "3/3 - 0s - 6ms/step - cyclic_mse: 182.5292 - loss: 179.0619 - val_cyclic_mse: 212.7022 - val_loss: 212.7022\n",
      "Epoch 144/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 181.2320 - loss: 175.4911 - val_cyclic_mse: 213.4097 - val_loss: 213.4097\n",
      "Epoch 145/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 181.4311 - loss: 183.3611 - val_cyclic_mse: 212.8410 - val_loss: 212.8409\n",
      "Epoch 146/300\n",
      "3/3 - 0s - 8ms/step - cyclic_mse: 179.4268 - loss: 178.4608 - val_cyclic_mse: 212.0728 - val_loss: 212.0728\n",
      "Epoch 147/300\n",
      "3/3 - 0s - 8ms/step - cyclic_mse: 181.1356 - loss: 192.0440 - val_cyclic_mse: 211.5688 - val_loss: 211.5688\n",
      "Epoch 148/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 181.9328 - loss: 186.9025 - val_cyclic_mse: 211.4530 - val_loss: 211.4530\n",
      "Epoch 149/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 182.0662 - loss: 177.1519 - val_cyclic_mse: 211.3992 - val_loss: 211.3992\n",
      "Epoch 150/300\n",
      "3/3 - 0s - 6ms/step - cyclic_mse: 181.7269 - loss: 183.1942 - val_cyclic_mse: 211.3969 - val_loss: 211.3969\n",
      "Epoch 151/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 182.4196 - loss: 184.9727 - val_cyclic_mse: 211.2692 - val_loss: 211.2692\n",
      "Epoch 152/300\n",
      "3/3 - 0s - 6ms/step - cyclic_mse: 182.1799 - loss: 177.9796 - val_cyclic_mse: 211.0334 - val_loss: 211.0334\n",
      "Epoch 153/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 181.6970 - loss: 184.5191 - val_cyclic_mse: 210.5577 - val_loss: 210.5577\n",
      "Epoch 154/300\n",
      "3/3 - 0s - 6ms/step - cyclic_mse: 182.4078 - loss: 180.2780 - val_cyclic_mse: 211.1977 - val_loss: 211.1978\n",
      "Epoch 155/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 181.7349 - loss: 173.8870 - val_cyclic_mse: 210.8749 - val_loss: 210.8749\n",
      "Epoch 156/300\n",
      "3/3 - 0s - 6ms/step - cyclic_mse: 181.4350 - loss: 185.3200 - val_cyclic_mse: 211.1148 - val_loss: 211.1147\n",
      "Epoch 157/300\n",
      "3/3 - 0s - 8ms/step - cyclic_mse: 182.2332 - loss: 174.0665 - val_cyclic_mse: 211.3757 - val_loss: 211.3757\n",
      "Epoch 158/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 181.5498 - loss: 180.8835 - val_cyclic_mse: 211.4080 - val_loss: 211.4080\n",
      "Epoch 159/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 181.8221 - loss: 181.2692 - val_cyclic_mse: 211.6483 - val_loss: 211.6483\n",
      "Epoch 160/300\n",
      "3/3 - 0s - 8ms/step - cyclic_mse: 181.3846 - loss: 187.0684 - val_cyclic_mse: 212.1920 - val_loss: 212.1920\n",
      "Epoch 161/300\n",
      "3/3 - 0s - 9ms/step - cyclic_mse: 182.0818 - loss: 175.5438 - val_cyclic_mse: 213.2208 - val_loss: 213.2208\n",
      "Epoch 162/300\n",
      "3/3 - 0s - 6ms/step - cyclic_mse: 180.1288 - loss: 177.2504 - val_cyclic_mse: 213.7546 - val_loss: 213.7546\n",
      "Epoch 163/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 182.0323 - loss: 184.5652 - val_cyclic_mse: 215.3075 - val_loss: 215.3075\n",
      "Epoch 164/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 182.6198 - loss: 179.5982 - val_cyclic_mse: 219.4986 - val_loss: 219.4986\n",
      "Epoch 165/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 183.6160 - loss: 178.3894 - val_cyclic_mse: 222.7085 - val_loss: 222.7085\n",
      "Epoch 166/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 182.5478 - loss: 183.3057 - val_cyclic_mse: 217.6386 - val_loss: 217.6386\n",
      "Epoch 167/300\n",
      "3/3 - 0s - 9ms/step - cyclic_mse: 183.0959 - loss: 182.9181 - val_cyclic_mse: 213.5458 - val_loss: 213.5458\n",
      "Epoch 168/300\n",
      "3/3 - 0s - 9ms/step - cyclic_mse: 182.1625 - loss: 187.2402 - val_cyclic_mse: 212.4765 - val_loss: 212.4765\n",
      "Epoch 169/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 182.4458 - loss: 187.6702 - val_cyclic_mse: 212.0113 - val_loss: 212.0113\n",
      "Epoch 170/300\n",
      "3/3 - 0s - 8ms/step - cyclic_mse: 181.4588 - loss: 186.6067 - val_cyclic_mse: 211.6262 - val_loss: 211.6262\n",
      "Epoch 171/300\n",
      "3/3 - 0s - 8ms/step - cyclic_mse: 181.2343 - loss: 179.4116 - val_cyclic_mse: 211.6200 - val_loss: 211.6200\n",
      "Epoch 172/300\n",
      "3/3 - 0s - 6ms/step - cyclic_mse: 181.6124 - loss: 185.8604 - val_cyclic_mse: 211.4167 - val_loss: 211.4167\n",
      "Epoch 173/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 182.1028 - loss: 180.4552 - val_cyclic_mse: 211.5289 - val_loss: 211.5289\n",
      "Epoch 174/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 182.1817 - loss: 183.8504 - val_cyclic_mse: 211.5007 - val_loss: 211.5007\n",
      "Epoch 175/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 182.0948 - loss: 179.7160 - val_cyclic_mse: 211.5238 - val_loss: 211.5238\n",
      "Epoch 176/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 181.1451 - loss: 184.2052 - val_cyclic_mse: 211.5513 - val_loss: 211.5513\n",
      "Epoch 177/300\n",
      "3/3 - 0s - 8ms/step - cyclic_mse: 182.0196 - loss: 190.1187 - val_cyclic_mse: 211.7050 - val_loss: 211.7050\n",
      "Epoch 178/300\n",
      "3/3 - 0s - 6ms/step - cyclic_mse: 181.5719 - loss: 179.7582 - val_cyclic_mse: 211.7413 - val_loss: 211.7413\n",
      "Epoch 179/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 182.6850 - loss: 188.8422 - val_cyclic_mse: 211.6721 - val_loss: 211.6721\n",
      "Epoch 180/300\n",
      "3/3 - 0s - 11ms/step - cyclic_mse: 181.5317 - loss: 179.6492 - val_cyclic_mse: 212.3887 - val_loss: 212.3887\n",
      "Epoch 181/300\n",
      "3/3 - 0s - 8ms/step - cyclic_mse: 181.0595 - loss: 185.0278 - val_cyclic_mse: 212.7586 - val_loss: 212.7586\n",
      "Epoch 182/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 183.0030 - loss: 177.6604 - val_cyclic_mse: 214.3221 - val_loss: 214.3221\n",
      "Epoch 183/300\n",
      "3/3 - 0s - 8ms/step - cyclic_mse: 181.4302 - loss: 185.1141 - val_cyclic_mse: 214.6035 - val_loss: 214.6035\n",
      "Epoch 184/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 180.3013 - loss: 184.2234 - val_cyclic_mse: 213.9498 - val_loss: 213.9498\n",
      "Epoch 185/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 181.6324 - loss: 178.8852 - val_cyclic_mse: 213.5914 - val_loss: 213.5914\n",
      "Epoch 186/300\n",
      "3/3 - 0s - 8ms/step - cyclic_mse: 180.5098 - loss: 180.8071 - val_cyclic_mse: 213.1806 - val_loss: 213.1806\n",
      "Epoch 187/300\n",
      "3/3 - 0s - 8ms/step - cyclic_mse: 181.1849 - loss: 184.4608 - val_cyclic_mse: 213.6679 - val_loss: 213.6679\n",
      "Epoch 188/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 181.9283 - loss: 177.5576 - val_cyclic_mse: 213.5777 - val_loss: 213.5778\n",
      "Epoch 189/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 182.8078 - loss: 181.8757 - val_cyclic_mse: 212.7830 - val_loss: 212.7830\n",
      "Epoch 190/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 181.7676 - loss: 175.5008 - val_cyclic_mse: 214.2353 - val_loss: 214.2352\n",
      "Epoch 191/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 183.8070 - loss: 182.6835 - val_cyclic_mse: 214.8374 - val_loss: 214.8374\n",
      "Epoch 192/300\n",
      "3/3 - 0s - 6ms/step - cyclic_mse: 180.5148 - loss: 181.5604 - val_cyclic_mse: 215.1236 - val_loss: 215.1236\n",
      "Epoch 193/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 183.3824 - loss: 177.4296 - val_cyclic_mse: 214.5883 - val_loss: 214.5883\n",
      "Epoch 194/300\n",
      "3/3 - 0s - 6ms/step - cyclic_mse: 181.1913 - loss: 185.2197 - val_cyclic_mse: 213.6533 - val_loss: 213.6533\n",
      "Epoch 195/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 181.2250 - loss: 184.4388 - val_cyclic_mse: 212.8032 - val_loss: 212.8032\n",
      "Epoch 196/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 180.9813 - loss: 179.0639 - val_cyclic_mse: 212.6523 - val_loss: 212.6523\n",
      "Epoch 197/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 180.7932 - loss: 185.9803 - val_cyclic_mse: 211.8511 - val_loss: 211.8511\n",
      "Epoch 198/300\n",
      "3/3 - 0s - 8ms/step - cyclic_mse: 180.7880 - loss: 181.5734 - val_cyclic_mse: 211.8817 - val_loss: 211.8817\n",
      "Epoch 199/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 184.4585 - loss: 186.9536 - val_cyclic_mse: 212.8975 - val_loss: 212.8975\n",
      "Epoch 200/300\n",
      "3/3 - 0s - 6ms/step - cyclic_mse: 181.4816 - loss: 183.5285 - val_cyclic_mse: 213.4961 - val_loss: 213.4961\n",
      "Epoch 201/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 181.9589 - loss: 187.9747 - val_cyclic_mse: 213.1563 - val_loss: 213.1563\n",
      "Epoch 202/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 181.4422 - loss: 174.9141 - val_cyclic_mse: 212.3276 - val_loss: 212.3276\n",
      "Epoch 203/300\n",
      "3/3 - 0s - 8ms/step - cyclic_mse: 181.9434 - loss: 190.5717 - val_cyclic_mse: 211.7933 - val_loss: 211.7933\n",
      "Epoch 204/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 181.0007 - loss: 177.1216 - val_cyclic_mse: 211.4931 - val_loss: 211.4931\n",
      "Epoch 205/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 182.7896 - loss: 174.6822 - val_cyclic_mse: 211.3757 - val_loss: 211.3757\n",
      "Epoch 206/300\n",
      "3/3 - 0s - 6ms/step - cyclic_mse: 182.3705 - loss: 182.0456 - val_cyclic_mse: 211.3795 - val_loss: 211.3795\n",
      "Epoch 207/300\n",
      "3/3 - 0s - 8ms/step - cyclic_mse: 181.7840 - loss: 179.8632 - val_cyclic_mse: 211.5574 - val_loss: 211.5574\n",
      "Epoch 208/300\n",
      "3/3 - 0s - 8ms/step - cyclic_mse: 181.8428 - loss: 180.9161 - val_cyclic_mse: 211.8129 - val_loss: 211.8129\n",
      "Epoch 209/300\n",
      "3/3 - 0s - 5ms/step - cyclic_mse: 181.3217 - loss: 175.9870 - val_cyclic_mse: 212.9631 - val_loss: 212.9631\n",
      "Epoch 210/300\n",
      "3/3 - 0s - 9ms/step - cyclic_mse: 180.7107 - loss: 181.1279 - val_cyclic_mse: 213.3725 - val_loss: 213.3725\n",
      "Epoch 211/300\n",
      "3/3 - 0s - 8ms/step - cyclic_mse: 181.1704 - loss: 179.5897 - val_cyclic_mse: 213.0990 - val_loss: 213.0990\n",
      "Epoch 212/300\n",
      "3/3 - 0s - 6ms/step - cyclic_mse: 181.2328 - loss: 182.7187 - val_cyclic_mse: 213.2603 - val_loss: 213.2603\n",
      "Epoch 213/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 183.7350 - loss: 178.3127 - val_cyclic_mse: 215.7390 - val_loss: 215.7390\n",
      "Epoch 214/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 182.2284 - loss: 192.4945 - val_cyclic_mse: 215.2280 - val_loss: 215.2280\n",
      "Epoch 215/300\n",
      "3/3 - 0s - 6ms/step - cyclic_mse: 182.5163 - loss: 197.3873 - val_cyclic_mse: 216.9762 - val_loss: 216.9762\n",
      "Epoch 216/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 181.1490 - loss: 184.1683 - val_cyclic_mse: 216.3441 - val_loss: 216.3441\n",
      "Epoch 217/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 185.3533 - loss: 187.4021 - val_cyclic_mse: 213.9025 - val_loss: 213.9025\n",
      "Epoch 218/300\n",
      "3/3 - 0s - 8ms/step - cyclic_mse: 181.6557 - loss: 188.5333 - val_cyclic_mse: 212.7673 - val_loss: 212.7673\n",
      "Epoch 219/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 180.5820 - loss: 177.6472 - val_cyclic_mse: 212.1704 - val_loss: 212.1704\n",
      "Epoch 220/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 181.0747 - loss: 180.5294 - val_cyclic_mse: 211.7025 - val_loss: 211.7025\n",
      "Epoch 221/300\n",
      "3/3 - 0s - 8ms/step - cyclic_mse: 181.4443 - loss: 179.9839 - val_cyclic_mse: 211.6610 - val_loss: 211.6610\n",
      "Epoch 222/300\n",
      "3/3 - 0s - 8ms/step - cyclic_mse: 183.1790 - loss: 181.6271 - val_cyclic_mse: 211.3909 - val_loss: 211.3909\n",
      "Epoch 223/300\n",
      "3/3 - 0s - 6ms/step - cyclic_mse: 182.1220 - loss: 181.0603 - val_cyclic_mse: 211.3760 - val_loss: 211.3760\n",
      "Epoch 224/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 181.9828 - loss: 180.4075 - val_cyclic_mse: 211.3841 - val_loss: 211.3841\n",
      "Epoch 225/300\n",
      "3/3 - 0s - 9ms/step - cyclic_mse: 182.5523 - loss: 189.7806 - val_cyclic_mse: 211.6723 - val_loss: 211.6723\n",
      "Epoch 226/300\n",
      "3/3 - 0s - 10ms/step - cyclic_mse: 182.5850 - loss: 186.7946 - val_cyclic_mse: 212.2081 - val_loss: 212.2081\n",
      "Epoch 227/300\n",
      "3/3 - 0s - 8ms/step - cyclic_mse: 182.4174 - loss: 190.9222 - val_cyclic_mse: 212.9395 - val_loss: 212.9395\n",
      "Epoch 228/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 179.6816 - loss: 178.1180 - val_cyclic_mse: 214.2378 - val_loss: 214.2378\n",
      "Epoch 229/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 180.9782 - loss: 180.7239 - val_cyclic_mse: 218.5817 - val_loss: 218.5817\n",
      "Epoch 230/300\n",
      "3/3 - 0s - 9ms/step - cyclic_mse: 182.4330 - loss: 181.5302 - val_cyclic_mse: 219.7103 - val_loss: 219.7102\n",
      "Epoch 231/300\n",
      "3/3 - 0s - 8ms/step - cyclic_mse: 181.3038 - loss: 182.0199 - val_cyclic_mse: 224.8288 - val_loss: 224.8288\n",
      "Epoch 232/300\n",
      "3/3 - 0s - 6ms/step - cyclic_mse: 185.1984 - loss: 186.9568 - val_cyclic_mse: 224.5373 - val_loss: 224.5374\n",
      "Epoch 233/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 182.2194 - loss: 181.9806 - val_cyclic_mse: 225.2802 - val_loss: 225.2802\n",
      "Epoch 234/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 181.9307 - loss: 184.8774 - val_cyclic_mse: 220.6063 - val_loss: 220.6063\n",
      "Epoch 235/300\n",
      "3/3 - 0s - 6ms/step - cyclic_mse: 182.2702 - loss: 187.6492 - val_cyclic_mse: 214.8378 - val_loss: 214.8378\n",
      "Epoch 236/300\n",
      "3/3 - 0s - 8ms/step - cyclic_mse: 180.5289 - loss: 180.1154 - val_cyclic_mse: 211.9443 - val_loss: 211.9443\n",
      "Epoch 237/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 180.7240 - loss: 181.8665 - val_cyclic_mse: 211.4191 - val_loss: 211.4191\n",
      "Epoch 238/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 181.6170 - loss: 186.5406 - val_cyclic_mse: 210.7271 - val_loss: 210.7271\n",
      "Epoch 239/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 182.4297 - loss: 187.2947 - val_cyclic_mse: 210.0535 - val_loss: 210.0535\n",
      "Epoch 240/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 182.1443 - loss: 181.1093 - val_cyclic_mse: 209.9449 - val_loss: 209.9448\n",
      "Epoch 241/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 182.5310 - loss: 182.3622 - val_cyclic_mse: 210.1253 - val_loss: 210.1253\n",
      "Epoch 242/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 182.1512 - loss: 181.0459 - val_cyclic_mse: 210.8949 - val_loss: 210.8949\n",
      "Epoch 243/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 182.7718 - loss: 178.4260 - val_cyclic_mse: 211.3908 - val_loss: 211.3908\n",
      "Epoch 244/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 181.3545 - loss: 187.2419 - val_cyclic_mse: 211.7108 - val_loss: 211.7108\n",
      "Epoch 245/300\n",
      "3/3 - 0s - 8ms/step - cyclic_mse: 181.1453 - loss: 178.1669 - val_cyclic_mse: 212.3653 - val_loss: 212.3653\n",
      "Epoch 246/300\n",
      "3/3 - 0s - 8ms/step - cyclic_mse: 181.1267 - loss: 171.9055 - val_cyclic_mse: 213.9816 - val_loss: 213.9816\n",
      "Epoch 247/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 181.9398 - loss: 184.4589 - val_cyclic_mse: 215.0289 - val_loss: 215.0289\n",
      "Epoch 248/300\n",
      "3/3 - 0s - 8ms/step - cyclic_mse: 181.8235 - loss: 186.0509 - val_cyclic_mse: 214.6197 - val_loss: 214.6196\n",
      "Epoch 249/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 181.5293 - loss: 181.9091 - val_cyclic_mse: 214.8414 - val_loss: 214.8415\n",
      "Epoch 250/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 181.5094 - loss: 182.8438 - val_cyclic_mse: 215.6155 - val_loss: 215.6155\n",
      "Epoch 251/300\n",
      "3/3 - 0s - 8ms/step - cyclic_mse: 182.3910 - loss: 185.7907 - val_cyclic_mse: 215.4518 - val_loss: 215.4518\n",
      "Epoch 252/300\n",
      "3/3 - 0s - 6ms/step - cyclic_mse: 182.1173 - loss: 178.3462 - val_cyclic_mse: 217.5281 - val_loss: 217.5281\n",
      "Epoch 253/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 182.3922 - loss: 177.7144 - val_cyclic_mse: 219.5375 - val_loss: 219.5375\n",
      "Epoch 254/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 182.1588 - loss: 173.1668 - val_cyclic_mse: 217.2558 - val_loss: 217.2558\n",
      "Epoch 255/300\n",
      "3/3 - 0s - 8ms/step - cyclic_mse: 184.4695 - loss: 181.0012 - val_cyclic_mse: 216.2637 - val_loss: 216.2637\n",
      "Epoch 256/300\n",
      "3/3 - 0s - 8ms/step - cyclic_mse: 181.1922 - loss: 177.2173 - val_cyclic_mse: 214.7224 - val_loss: 214.7224\n",
      "Epoch 257/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 182.0377 - loss: 182.4257 - val_cyclic_mse: 213.3463 - val_loss: 213.3463\n",
      "Epoch 258/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 182.0793 - loss: 191.3078 - val_cyclic_mse: 213.6626 - val_loss: 213.6626\n",
      "Epoch 259/300\n",
      "3/3 - 0s - 8ms/step - cyclic_mse: 181.5277 - loss: 185.1374 - val_cyclic_mse: 211.8959 - val_loss: 211.8959\n",
      "Epoch 260/300\n",
      "3/3 - 0s - 8ms/step - cyclic_mse: 181.3201 - loss: 181.4506 - val_cyclic_mse: 211.3776 - val_loss: 211.3776\n",
      "Epoch 261/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 180.7092 - loss: 185.1775 - val_cyclic_mse: 209.3343 - val_loss: 209.3343\n",
      "Epoch 262/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 181.4470 - loss: 185.4917 - val_cyclic_mse: 208.7940 - val_loss: 208.7940\n",
      "Epoch 263/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 182.9213 - loss: 183.1136 - val_cyclic_mse: 208.8864 - val_loss: 208.8864\n",
      "Epoch 264/300\n",
      "3/3 - 0s - 11ms/step - cyclic_mse: 182.2045 - loss: 187.6110 - val_cyclic_mse: 208.7197 - val_loss: 208.7197\n",
      "Epoch 265/300\n",
      "3/3 - 0s - 8ms/step - cyclic_mse: 182.9709 - loss: 184.3516 - val_cyclic_mse: 208.5523 - val_loss: 208.5523\n",
      "Epoch 266/300\n",
      "3/3 - 0s - 9ms/step - cyclic_mse: 182.8027 - loss: 183.5532 - val_cyclic_mse: 208.5760 - val_loss: 208.5760\n",
      "Epoch 267/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 183.0388 - loss: 185.1086 - val_cyclic_mse: 208.8585 - val_loss: 208.8585\n",
      "Epoch 268/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 183.2496 - loss: 184.8700 - val_cyclic_mse: 208.8974 - val_loss: 208.8973\n",
      "Epoch 269/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 182.8233 - loss: 180.1055 - val_cyclic_mse: 209.7044 - val_loss: 209.7044\n",
      "Epoch 270/300\n",
      "3/3 - 0s - 8ms/step - cyclic_mse: 182.7699 - loss: 182.9532 - val_cyclic_mse: 210.6455 - val_loss: 210.6455\n",
      "Epoch 271/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 181.0422 - loss: 177.4858 - val_cyclic_mse: 211.3771 - val_loss: 211.3771\n",
      "Epoch 272/300\n",
      "3/3 - 0s - 8ms/step - cyclic_mse: 182.4476 - loss: 185.1046 - val_cyclic_mse: 212.4631 - val_loss: 212.4631\n",
      "Epoch 273/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 180.9366 - loss: 175.1958 - val_cyclic_mse: 214.3938 - val_loss: 214.3938\n",
      "Epoch 274/300\n",
      "3/3 - 0s - 8ms/step - cyclic_mse: 181.0864 - loss: 183.9031 - val_cyclic_mse: 216.6290 - val_loss: 216.6290\n",
      "Epoch 275/300\n",
      "3/3 - 0s - 8ms/step - cyclic_mse: 181.3645 - loss: 186.3754 - val_cyclic_mse: 215.2057 - val_loss: 215.2057\n",
      "Epoch 276/300\n",
      "3/3 - 0s - 6ms/step - cyclic_mse: 180.9830 - loss: 178.0694 - val_cyclic_mse: 215.6049 - val_loss: 215.6049\n",
      "Epoch 277/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 181.9537 - loss: 181.5233 - val_cyclic_mse: 214.1216 - val_loss: 214.1216\n",
      "Epoch 278/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 181.8997 - loss: 186.3201 - val_cyclic_mse: 211.9202 - val_loss: 211.9202\n",
      "Epoch 279/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 181.2372 - loss: 174.7435 - val_cyclic_mse: 211.6296 - val_loss: 211.6296\n",
      "Epoch 280/300\n",
      "3/3 - 0s - 8ms/step - cyclic_mse: 182.6423 - loss: 178.5104 - val_cyclic_mse: 211.6666 - val_loss: 211.6666\n",
      "Epoch 281/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 181.4514 - loss: 184.2985 - val_cyclic_mse: 211.7197 - val_loss: 211.7197\n",
      "Epoch 282/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 181.1080 - loss: 180.4175 - val_cyclic_mse: 211.8298 - val_loss: 211.8298\n",
      "Epoch 283/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 181.5471 - loss: 182.2465 - val_cyclic_mse: 211.8462 - val_loss: 211.8462\n",
      "Epoch 284/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 181.5537 - loss: 184.3285 - val_cyclic_mse: 212.8962 - val_loss: 212.8962\n",
      "Epoch 285/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 181.8307 - loss: 188.4739 - val_cyclic_mse: 216.2765 - val_loss: 216.2765\n",
      "Epoch 286/300\n",
      "3/3 - 0s - 8ms/step - cyclic_mse: 181.0499 - loss: 177.6901 - val_cyclic_mse: 220.3860 - val_loss: 220.3860\n",
      "Epoch 287/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 181.9000 - loss: 173.2260 - val_cyclic_mse: 222.3282 - val_loss: 222.3282\n",
      "Epoch 288/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 184.0094 - loss: 188.5643 - val_cyclic_mse: 224.7543 - val_loss: 224.7543\n",
      "Epoch 289/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 183.4451 - loss: 190.7109 - val_cyclic_mse: 219.9428 - val_loss: 219.9428\n",
      "Epoch 290/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 184.1671 - loss: 186.7942 - val_cyclic_mse: 213.5910 - val_loss: 213.5910\n",
      "Epoch 291/300\n",
      "3/3 - 0s - 6ms/step - cyclic_mse: 180.6289 - loss: 184.0944 - val_cyclic_mse: 211.8142 - val_loss: 211.8142\n",
      "Epoch 292/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 182.2382 - loss: 183.0680 - val_cyclic_mse: 211.3869 - val_loss: 211.3870\n",
      "Epoch 293/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 182.2748 - loss: 182.3610 - val_cyclic_mse: 209.5744 - val_loss: 209.5744\n",
      "Epoch 294/300\n",
      "3/3 - 0s - 6ms/step - cyclic_mse: 182.8561 - loss: 184.7616 - val_cyclic_mse: 208.5833 - val_loss: 208.5833\n",
      "Epoch 295/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 183.1720 - loss: 184.2006 - val_cyclic_mse: 208.2801 - val_loss: 208.2801\n",
      "Epoch 296/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 183.5504 - loss: 182.9615 - val_cyclic_mse: 208.1197 - val_loss: 208.1197\n",
      "Epoch 297/300\n",
      "3/3 - 0s - 11ms/step - cyclic_mse: 182.8183 - loss: 180.1277 - val_cyclic_mse: 208.1043 - val_loss: 208.1044\n",
      "Epoch 298/300\n",
      "3/3 - 0s - 6ms/step - cyclic_mse: 183.2268 - loss: 184.3446 - val_cyclic_mse: 208.1188 - val_loss: 208.1188\n",
      "Epoch 299/300\n",
      "3/3 - 0s - 7ms/step - cyclic_mse: 182.6403 - loss: 179.3837 - val_cyclic_mse: 208.1677 - val_loss: 208.1677\n",
      "Epoch 300/300\n",
      "3/3 - 0s - 6ms/step - cyclic_mse: 183.0953 - loss: 184.7225 - val_cyclic_mse: 208.2916 - val_loss: 208.2916\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1caa1b43c10>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "EPOCHS = 300\n",
    "model.fit(train_dataset, epochs=EPOCHS, validation_data=validation_dataset, verbose=2)\n",
    "# the following printed results are just for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "52b2c317-b388-4254-b6b7-3b44c859f26f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.save('B:/jupyter_notebook/localization/local_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b3626ed5-76ea-46d6-9933-5189cbf99638",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ reshape_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">39</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv1d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">39</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)               │             <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling1d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)               │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)               │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv1d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">400</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling1d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">161</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ lambda_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ reshape_1 (\u001b[38;5;33mReshape\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m39\u001b[0m, \u001b[38;5;34m13\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv1d_2 (\u001b[38;5;33mConv1D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m39\u001b[0m, \u001b[38;5;34m8\u001b[0m)               │             \u001b[38;5;34m320\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling1d_2 (\u001b[38;5;33mMaxPooling1D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m8\u001b[0m)               │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m8\u001b[0m)               │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv1d_3 (\u001b[38;5;33mConv1D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m16\u001b[0m)              │             \u001b[38;5;34m400\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling1d_3 (\u001b[38;5;33mMaxPooling1D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m16\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m16\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m160\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │             \u001b[38;5;34m161\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ lambda_1 (\u001b[38;5;33mLambda\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │               \u001b[38;5;34m0\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,645</span> (10.34 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,645\u001b[0m (10.34 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">881</span> (3.44 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m881\u001b[0m (3.44 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,764</span> (6.89 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m1,764\u001b[0m (6.89 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39d8391-731b-4a1a-a77e-872457f4f6ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
